% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpt2.R
\name{gpt2}
\alias{gpt2}
\alias{gpt2_from_config}
\alias{gpt2_from_pretrained}
\title{GPT2}
\usage{
gpt2(
  vocab_size = 50257,
  n_embd = 768,
  n_head = 12,
  n_layer = 12,
  max_pos = 1024,
  pdrop = 0.1
)

gpt2_from_config(identifier, revision = "main")

gpt2_from_pretrained(identifier, revision = "main")
}
\arguments{
\item{vocab_size}{An optional integer indicating the size of the vocabulary or the number of unique tokens in the input data.}

\item{n_embd}{An integer specifying the Dimensionality of the embeddings and hidden states.}

\item{n_head}{An integer representing the number of attention heads in each attention layer in the Transformer encoder.}

\item{n_layer}{An integer indicating the umber of hidden layers in the Transformer encoder.}

\item{max_pos}{An integer specifying the maximum sequence length that this model might ever be used with.}

\item{pdrop}{The dropout rate used in a few locations, such as after the embeddings,
attention layers and residual connections.}

\item{identifier}{A string representing the identifier or name of the pre-trained model in the Hugging Face model hub.}

\item{revision}{A string specifying the revision or version of the pre-trained model in the Hugging Face model hub.}
}
\value{
An initialized \code{\link[torch:nn_module]{torch::nn_module()}}.
}
\description{
Initializes a gpt2-type model
}
\section{Functions}{
\itemize{
\item \code{gpt2_from_config()}: Initializes a gpt2 model using a configuration defined in HF Hub

\item \code{gpt2_from_pretrained()}: Initializes the gpt2 model and load pre-trained weights from HF hub.

}}
