% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama}
\alias{llama}
\alias{llama_from_config}
\alias{llama_from_pretrained}
\title{llama}
\usage{
llama(
  vocab_size = 50432,
  n_embd = 6144,
  n_inter = 11008,
  n_head = 64,
  n_layer = 44,
  max_pos = 2048,
  rmsnorm_eps = 1e-06,
  rope_base = 10000
)

llama_from_config(identifier, revision = "main")

llama_from_pretrained(identifier, revision = "main")
}
\arguments{
\item{vocab_size}{An integer indicating the size of the vocabulary or the number
of unique tokens in the input data.}

\item{n_embd}{An integer specifying the dimensionality of the embedding vectors.}

\item{n_inter}{An integer specifying the size of the intermediate layer in the MLP}

\item{n_head}{An integer representing the number of attention heads in the
multi-head attention mechanism.}

\item{n_layer}{An integer indicating the number of layers in the deep learning model.}

\item{max_pos}{An integer specifying the maximum position encoding value or
the maximum sequence length.}

\item{rmsnorm_eps}{The epsilon used by the rms normalization layers.}

\item{rope_base}{The base period of the RoPE embeddings.}

\item{identifier}{A string representing the identifier or name of the pre-trained
model in the Hugging Face model hub.}

\item{revision}{A string specifying the revision or version of the pre-trained
model in the Hugging Face model hub.}
}
\value{
An initialized \code{\link[torch:nn_module]{torch::nn_module()}}.
}
\description{
Initializes a llama like model
}
\section{Functions}{
\itemize{
\item \code{llama_from_config()}: Initializes a llama model using a configuration defined in HF Hub

\item \code{llama_from_pretrained()}: Initializes the llama model and load pre-trained weights from HF hub.

}}
